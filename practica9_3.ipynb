{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practica9_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iubordei/Ahorcado/blob/master/practica9_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXROQV9uZ2n"
      },
      "source": [
        "# Práctica 9 Parte 3: Desarrollando un modelo de lenguaje para generar texto\n",
        "\n",
        "Un modelo de lenguaje puede predecir la siguiente palabra de una secuencia basándose en palabras observadas anteriormente. Las redes neuronales son el método más utilizado para desarrollar este tipo de modelos porque pueden usar una representación donde palabras con significados similares tienen representaciones similares. \n",
        "\n",
        "En esta parte de la práctica vamos a ver cómo generar uno de esos modelos. \n",
        "\n",
        "Este notebook está basado en el libro Deep Learning for Natural Language Processing de Jason Brownlee. \n",
        "\n",
        "Es importante que tengas activado el uso de **GPU** en el notebook de colab (menú Edit -> Notebook Settings -> Hardware accelerator)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fHn2VKwO0E"
      },
      "source": [
        "## La República de Platón\n",
        "\n",
        "Nuestro modelo de lenguaje va a estar basado en la república de Platón. Este libro está estructurado en forma de una conversación que trata el tema del orden y la justicia dentro de una ciudad. El texto completo está disponible para el dominio público dentro del [proyecto Gutenberg](http://www.gutenberg.org/).\n",
        "\n",
        "Este libro de Platón está disponible en varios formatos en el [proyecto Gutenberg](http://www.gutenberg.org/cache/epub/1497/pg1497.txt). La versión que nos interesa a nosotros es la versión ASCII del libro. Con la siguiente instrucción puedes descargar el libro donde se han eliminado la portada y la contraportada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmhqeiuuFYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a78897-ff45-4c1f-a84f-f61818ec323a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt -O republic.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-15 08:33:08--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/republic.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 657826 (642K) [text/plain]\n",
            "Saving to: ‘republic.txt’\n",
            "\n",
            "republic.txt        100%[===================>] 642.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-15 08:33:08 (17.5 MB/s) - ‘republic.txt’ saved [657826/657826]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_hckdhxWdz"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Vamos a preparar los datos para construir nuestro modelo. \n",
        "\n",
        "### Revisando el texto\n",
        "\n",
        "Vamos a comenzar revisando parte del texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn3g3XJYxSTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6589c453-fc56-410f-bdbe-fe0c66bca474"
      },
      "source": [
        "!head -30 republic.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I.\r\n",
            "\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\n",
            "distance as we were starting on our way home, and told his servant to\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\n",
            "\r\n",
            "I turned round, and asked him where his master was.\r\n",
            "\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\n",
            "\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\n",
            "of Nicias, and several others who had been at the procession.\r\n",
            "\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\n",
            "companion are already on your way to the city.\r\n",
            "\r\n",
            "You are not far wrong, I said.\r\n",
            "\r\n",
            "But do you see, he rejoined, how many we are?\r\n",
            "\r\n",
            "Of course.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4uFv8RWzVn-"
      },
      "source": [
        "A partir de un rápido vistazo al fragmento de texto anterior podemos ver ciertas cuestiones que tendremos que procesar:\n",
        "- Las cabeceras de los capítulos.\n",
        "- Muchos signos de puntuación.\n",
        "- Nombres extraños.\n",
        "- Algunos monólogos muy largos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfXxOz0GQT"
      },
      "source": [
        "### Cargando el texto\n",
        "\n",
        "El primer paso consiste en cargar el texto en memoria. Podemos desarrollar una pequeña función que se encargue de esto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dDTRFi4xmBr"
      },
      "source": [
        "def load_doc(filename):\n",
        "    # Abrimos el fichero en modo lectura\n",
        "    file = open(filename,'r')\n",
        "    # Leemos el texto completo\n",
        "    text = file.read()\n",
        "    # Cerramos el fichero\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SdKtPFM0aZ-"
      },
      "source": [
        "Usando dicha función podemos cargar nuestro fichero del siguiente modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_oUjl_Q0Zgn"
      },
      "source": [
        "in_filename = 'republic.txt'\n",
        "doc = load_doc(in_filename)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfi8WHI0kKb"
      },
      "source": [
        "Ahora podemos mostrar parte de dicho texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZRb4q6W0jQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ee2293-e72e-4178-afbe-9e27a4876a68"
      },
      "source": [
        "print(doc[:200])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj4PZ32G0reo"
      },
      "source": [
        "### Limpiando el texto\n",
        "\n",
        "Ahora necesitamos transformar el texto en bruto a una secuencia de tokens (o palabras) que podamos usar para entrenar nuestro modelo. \n",
        "\n",
        "Vamos a aplicar las siguientes operaciones para limpiar nuestro texto:\n",
        "- Reemplazar todas las ocurrencias de '-' con un espacio en blanco de manera que podamos partir mejor las palabras.\n",
        "- Partir las palabras basándonos en espacios en blanco.\n",
        "- Eliminar todos los símbolos de puntuación.\n",
        "- Eliminar todas las palabras que no son alfabéticas. \n",
        "- Normalizar todas las palabras a minúsculas.\n",
        "\n",
        "La mayoría de estas transformaciones tienen como objetivo reducir el tamaño del vocabulario. Un tamaño de vocabulario excesivamente grande es un problema cuando se intenta crear modelos de lenguaje. Vocabularios pequeños producen modelos más pequeños que se entrenan más rápidos.\n",
        "\n",
        "Vamos a implementar todas las operaciones de limpieza en la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsZiyRsE0niT"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # Reemplazar '--' con un espacio en blanco ' '\n",
        "    doc = doc.replace('--',' ')\n",
        "    # Partir palabras basándonos en espacios en blanco\n",
        "    tokens = doc.split()\n",
        "    # Vamos a escapar las palabras para poder filtrarlas por caracteres\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # Eliminamos los símbolos de puntuación\n",
        "    tokens = [re_punc.sub('',w) for w in tokens]\n",
        "    # Eliminamos elementos que nos son alfabéticos\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Convertimos a minúsculas\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVV2RsWB2kOD"
      },
      "source": [
        "Procedemos a limpiar nuestro documento y a continuación mostramos algunas estadísticas sobre nuestro vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUUCtGKP2jdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a598d8ad-6c81-4bce-f7df-5a996fe27077"
      },
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOBzRk232uAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05929ce6-1215-4fac-dd76-03943a748061"
      },
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvkoWbVh25cR"
      },
      "source": [
        "Es decir, nuestro modelo consta de una 7500 palabras. Este tamaño de vocabulario es pequeño y va a ser manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlhVwpz3CQi"
      },
      "source": [
        "### Guardando el texto limpio\n",
        "\n",
        "Vamos a organizar la larga lista de tokens en secuencias de 50 palabras de entrada y 1 palabra de salida (esto servirá para luego entrenar nuestro modelo). \n",
        "\n",
        "Este proceso lo implementamos con la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtEe2J7E21am"
      },
      "source": [
        "def organize_tokens(tokens,input_len=50,output_len=1):\n",
        "    length = input_len + output_len\n",
        "    sequences = list()\n",
        "    for i in range(length,len(tokens)):\n",
        "        # Elegimos la secuencia de tokens\n",
        "        seq = tokens[i-length:i]\n",
        "        # Convertimos la secuencia en una línea\n",
        "        line = ' '.join(seq)\n",
        "        # Almacenamos el resultado\n",
        "        sequences.append(line)\n",
        "    return sequences"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6upZchl37wM"
      },
      "source": [
        "Organizamos nuestros tokens. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpcwuFBp35XJ"
      },
      "source": [
        "lines = organize_tokens(tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXyDfPjA4Bfr"
      },
      "source": [
        "Ahora vamos a guardar las secuencias en un nuevo fichero para poder cargarlo en el futuro. Para ello nos definimos la siguiente función que guardará cada elemento de la secuencia en una línea del fichero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWxPoCvn4AfS"
      },
      "source": [
        "def save_doc(lines,filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename,'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtdjRviE4Wfd"
      },
      "source": [
        "Podemos llamar a la función anterior para guardar nuestro fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbncNaRo4T74"
      },
      "source": [
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(lines,out_filename)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGMZ3yW4m9O"
      },
      "source": [
        "Podemos ver parte de dicho fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hitWeMb84e1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83026dd0-8fb0-4be0-bb73-1efcbccdfa7d"
      },
      "source": [
        "!head -5 republic_sequences.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted\n",
            "i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with\n",
            "went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the\n",
            "down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Do4ht04u3Q"
      },
      "source": [
        "## Entrenando el modelo de lenguaje\n",
        "\n",
        "Vamosa  entrenar ahora nuestro modelo a partir de los datos que hemos preparado. Dicho modelo tendrá ciertas características:\n",
        "- Usará una representación para las palabras de manera que palabras diferentes con significados similares tendrán una representación similar.\n",
        "- La representación será aprendida al mismo tiempo que se aprende el modelo.\n",
        "- Aprenderá a predecir la probabilidad de la siguiente palabra a partir del contexto de las últimas 100 palabras.\n",
        "\n",
        "En concreto para implementar este modelo vamos a usar una capa de Embedding para aprender la representación de las palabras, y una red neuronal recurrente con capas LSTM para predecir nuevas palabras basándonos en el contexto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_vMHL65cx2"
      },
      "source": [
        "### Cargando las secuencias\n",
        "\n",
        "Podemos comenzar cargando las secuencias que hemos guardado anteriormente. En este caso este paso no sería necesario ya que el proceso de generación de las secuencias es bastante rápido, pero si estamos trabajando con un dataset más grande sí que puede ser conveniente. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mChmy3q04rJq"
      },
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au4s6vM7UED"
      },
      "source": [
        "### Codificando las secuencias\n",
        "\n",
        "Las capas de Embedding esperan que las secuencias de entrada estén compuestas de vectores de enteros. Para ello vamos a identificar cada palabra de nuestro vocabulario con un entero único y codificarlo en una secuencia de entrada. En el futuro cuando vayamos a realizar las predicciones tendremos que realizar el proceso inverso.\n",
        "\n",
        "Para llevar a cabo este proceso de tokenización vamos a usar la API de Keras del siguiente modo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVH8p7Cv52Zo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_odT9JZ710u"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEQoF2S478z9"
      },
      "source": [
        "Ahora podemos acceder a los identificadores de cada palabra usando el atributo ``word_index`` del objeto ``Tokenizer`` que hemos creado. \n",
        "\n",
        "Además debemos determinar el tamaño de nuestro vocabulario para definir la capa de embedding. En concreto, a las palabras de nuestro vocabulario se les han asignado valores entre 1 y el número total de palabras de nuestro vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byu4Vfe075Aj"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1 "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaF1yDTF9elO"
      },
      "source": [
        "### Secuencias de entrada y salida\n",
        "\n",
        "Una vez que tenemos codificadas nuestras secuencias tenemos que separarlas en elementos de entrada ($X$) y de salida ($y$). Después de realizar la separación debemos codificar cada palabra usando el método one-hot. Este proceso lo llevaremos a cabo mediante la función ``to_categorical()`` de Keras.\n",
        "Finalmente necesitamos especificar cómo de largas serán las secuencias de entrada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16bdwQAiYbtt"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from numpy import array "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SekaGTwl9eCl"
      },
      "source": [
        "sequences = array(sequences)\n",
        "X,y=sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y,num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUMEkkh-gSm"
      },
      "source": [
        "### Entrenando el modelo\n",
        "\n",
        "Ahora podemos definir nuestro modelo que constará de una capa de Embedding, seguida de dos capas LSTM y terminando con una red completamente conectada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f06QiAPsYdZW"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cs_7VIsS-ODn"
      },
      "source": [
        "def define_model(vocab_size,seq_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
        "    model.add(LSTM(100,return_sequences=True))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100,activation='relu'))\n",
        "    model.add(Dense(vocab_size,activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMo-mV9E_cco"
      },
      "source": [
        "Pasamos a entrenar nuestro modelo. Como este proceso es bastante costoso (incluso usando GPUs) en la siguiente sección se proporcionan los ficheros necesarios para usar el modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFxHVuAuCdzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a00aca5-773f-4789-af98-e4c8f233c069"
      },
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.fit(X,y,batch_size=128,epochs=100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "927/927 [==============================] - 47s 17ms/step - loss: 6.4985 - accuracy: 0.0576\n",
            "Epoch 2/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 5.7338 - accuracy: 0.1000\n",
            "Epoch 3/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 5.4764 - accuracy: 0.1274\n",
            "Epoch 4/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 5.2920 - accuracy: 0.1406\n",
            "Epoch 5/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 5.1544 - accuracy: 0.1550\n",
            "Epoch 6/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 5.0618 - accuracy: 0.1592\n",
            "Epoch 7/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 5.0797 - accuracy: 0.1545\n",
            "Epoch 8/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.9621 - accuracy: 0.1674\n",
            "Epoch 9/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.8789 - accuracy: 0.1704\n",
            "Epoch 10/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.8172 - accuracy: 0.1733\n",
            "Epoch 11/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.7276 - accuracy: 0.1789\n",
            "Epoch 12/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.6729 - accuracy: 0.1810\n",
            "Epoch 13/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.5993 - accuracy: 0.1829\n",
            "Epoch 14/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.5428 - accuracy: 0.1837\n",
            "Epoch 15/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.4717 - accuracy: 0.1897\n",
            "Epoch 16/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.4090 - accuracy: 0.1912\n",
            "Epoch 17/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.3446 - accuracy: 0.1931\n",
            "Epoch 18/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.2856 - accuracy: 0.1949\n",
            "Epoch 19/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.2289 - accuracy: 0.1976\n",
            "Epoch 20/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.1781 - accuracy: 0.1994\n",
            "Epoch 21/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.1385 - accuracy: 0.2030\n",
            "Epoch 22/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.0998 - accuracy: 0.2063\n",
            "Epoch 23/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.0402 - accuracy: 0.2113\n",
            "Epoch 24/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.0737 - accuracy: 0.2107\n",
            "Epoch 25/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 4.0117 - accuracy: 0.2138\n",
            "Epoch 26/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.9619 - accuracy: 0.2171\n",
            "Epoch 27/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.8994 - accuracy: 0.2237\n",
            "Epoch 28/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.8808 - accuracy: 0.2257\n",
            "Epoch 29/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.8351 - accuracy: 0.2304\n",
            "Epoch 30/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.8078 - accuracy: 0.2348\n",
            "Epoch 31/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.7605 - accuracy: 0.2364\n",
            "Epoch 32/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.7243 - accuracy: 0.2410\n",
            "Epoch 33/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.7760 - accuracy: 0.2411\n",
            "Epoch 34/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.7174 - accuracy: 0.2429\n",
            "Epoch 35/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.6385 - accuracy: 0.2502\n",
            "Epoch 36/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5970 - accuracy: 0.2548\n",
            "Epoch 37/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 3.5685 - accuracy: 0.2612\n",
            "Epoch 38/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5728 - accuracy: 0.2608\n",
            "Epoch 39/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5374 - accuracy: 0.2638\n",
            "Epoch 40/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4839 - accuracy: 0.2719\n",
            "Epoch 41/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.6115 - accuracy: 0.2618\n",
            "Epoch 42/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4980 - accuracy: 0.2712\n",
            "Epoch 43/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4735 - accuracy: 0.2772\n",
            "Epoch 44/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.6169 - accuracy: 0.2671\n",
            "Epoch 45/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5132 - accuracy: 0.2748\n",
            "Epoch 46/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4109 - accuracy: 0.2843\n",
            "Epoch 47/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4003 - accuracy: 0.2876\n",
            "Epoch 48/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3301 - accuracy: 0.2941\n",
            "Epoch 49/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3464 - accuracy: 0.2940\n",
            "Epoch 50/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4025 - accuracy: 0.2885\n",
            "Epoch 51/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4201 - accuracy: 0.2907\n",
            "Epoch 52/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3708 - accuracy: 0.2944\n",
            "Epoch 53/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3529 - accuracy: 0.2953\n",
            "Epoch 54/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3392 - accuracy: 0.3001\n",
            "Epoch 55/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4453 - accuracy: 0.2905\n",
            "Epoch 56/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.6766 - accuracy: 0.2684\n",
            "Epoch 57/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5810 - accuracy: 0.2725\n",
            "Epoch 58/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4813 - accuracy: 0.2830\n",
            "Epoch 59/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.5172 - accuracy: 0.2783\n",
            "Epoch 60/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.4583 - accuracy: 0.2795\n",
            "Epoch 61/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 3.3368 - accuracy: 0.2969\n",
            "Epoch 62/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.3144 - accuracy: 0.2999\n",
            "Epoch 63/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.2842 - accuracy: 0.3063\n",
            "Epoch 64/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.2336 - accuracy: 0.3102\n",
            "Epoch 65/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.2039 - accuracy: 0.3176\n",
            "Epoch 66/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.1610 - accuracy: 0.3209\n",
            "Epoch 67/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.0951 - accuracy: 0.3291\n",
            "Epoch 68/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 3.0716 - accuracy: 0.3337\n",
            "Epoch 69/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.0107 - accuracy: 0.3431\n",
            "Epoch 70/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9739 - accuracy: 0.3475\n",
            "Epoch 71/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9534 - accuracy: 0.3507\n",
            "Epoch 72/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.0075 - accuracy: 0.3444\n",
            "Epoch 73/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9539 - accuracy: 0.3541\n",
            "Epoch 74/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9405 - accuracy: 0.3572\n",
            "Epoch 75/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 2.9456 - accuracy: 0.3585\n",
            "Epoch 76/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.0030 - accuracy: 0.3481\n",
            "Epoch 77/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.1266 - accuracy: 0.3335\n",
            "Epoch 78/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 3.0112 - accuracy: 0.3501\n",
            "Epoch 79/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9701 - accuracy: 0.3539\n",
            "Epoch 80/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9841 - accuracy: 0.3521\n",
            "Epoch 81/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.9646 - accuracy: 0.3549\n",
            "Epoch 82/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.8591 - accuracy: 0.3703\n",
            "Epoch 83/100\n",
            "927/927 [==============================] - 15s 17ms/step - loss: 2.8235 - accuracy: 0.3738\n",
            "Epoch 84/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.8036 - accuracy: 0.3770\n",
            "Epoch 85/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.7410 - accuracy: 0.3860\n",
            "Epoch 86/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.7153 - accuracy: 0.3937\n",
            "Epoch 87/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.7055 - accuracy: 0.3946\n",
            "Epoch 88/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.6696 - accuracy: 0.3995\n",
            "Epoch 89/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 2.6519 - accuracy: 0.4029\n",
            "Epoch 90/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 2.6333 - accuracy: 0.4073\n",
            "Epoch 91/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 2.5874 - accuracy: 0.4136\n",
            "Epoch 92/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.7369 - accuracy: 0.3951\n",
            "Epoch 93/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.6172 - accuracy: 0.4141\n",
            "Epoch 94/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 2.5681 - accuracy: 0.4203\n",
            "Epoch 95/100\n",
            "927/927 [==============================] - 16s 18ms/step - loss: 2.5253 - accuracy: 0.4250\n",
            "Epoch 96/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.4928 - accuracy: 0.4320\n",
            "Epoch 97/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.4707 - accuracy: 0.4356\n",
            "Epoch 98/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.4616 - accuracy: 0.4346\n",
            "Epoch 99/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.4516 - accuracy: 0.4391\n",
            "Epoch 100/100\n",
            "927/927 [==============================] - 16s 17ms/step - loss: 2.5191 - accuracy: 0.4341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f49bb9e3510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EeUlPGxEifh"
      },
      "source": [
        "Una vez entrenado podemos guardar los pesos del modelo y el tokenizador. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKc5WObR_maO"
      },
      "source": [
        "model.save_weights('./model.h5', overwrite=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPdfgxs4Z5Qp"
      },
      "source": [
        "from pickle import dump"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzX5deoEuiw"
      },
      "source": [
        "dump(tokenizer, open('tokenizer.pkl','wb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnE1BOhEuIz"
      },
      "source": [
        "## Usando el modelo\n",
        "\n",
        "Como has podido ver en el paso anterior, el proceso de entrenar este tipo de modelos es muy costoso, por lo que puedes descargar los ficheros necesarios para usar el modelo desde el siguiente enlace. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJozw5qBFUoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb5f057-c1e4-468f-adb0-1d0782116e4b"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5 -O model.h5\n",
        "!wget https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl -O tokenizer.pkl"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-15 09:07:32--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tpu_model.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101248 (4.9M) [application/octet-stream]\n",
            "Saving to: ‘model.h5’\n",
            "\n",
            "model.h5            100%[===================>]   4.86M  29.4MB/s    in 0.2s    \n",
            "\n",
            "2021-05-15 09:07:33 (29.4 MB/s) - ‘model.h5’ saved [5101248/5101248]\n",
            "\n",
            "--2021-05-15 09:07:33--  https://raw.githubusercontent.com/ts1819/datasets/master/practica5/tokenizer.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353379 (345K) [application/octet-stream]\n",
            "Saving to: ‘tokenizer.pkl’\n",
            "\n",
            "tokenizer.pkl       100%[===================>] 345.10K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-05-15 09:07:34 (14.2 MB/s) - ‘tokenizer.pkl’ saved [353379/353379]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbxpdyp4FZ0t"
      },
      "source": [
        "### Cargando los datos\n",
        "\n",
        "Comenzamos cargando nuestros datos al igual que antes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwREuZMiFfVT"
      },
      "source": [
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIu2LzlFl5v"
      },
      "source": [
        "Necesitamos este texto para elegir una secuencia de inicio que será la entrada para nuestro modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-IslrBxFlnA"
      },
      "source": [
        "seq_length = len(lines[0].split())-1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCQcIN_Fxr2"
      },
      "source": [
        "### Cargando el modelo\n",
        "\n",
        "Vamos a cargar el modelo y a fijar los pesos. Notar que para este paso ya no necesitamos el uso de TPU, y que el modelo podría ser usado en cualquier ordenador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usmQZBvYF7Q1"
      },
      "source": [
        "model = define_model(vocab_size,seq_length)\n",
        "model.load_weights('./model.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvlOps3GG05"
      },
      "source": [
        "También necesitamos cargar el tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfEqSzUIYimc"
      },
      "source": [
        "from pickle import load"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xib_xurDGIpL"
      },
      "source": [
        "tokenizer = load(open('tokenizer.pkl','rb'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XrPda8NGUv7"
      },
      "source": [
        "### Generando texto\n",
        "\n",
        "El primer paso para generar el texto consiste en preparar una entrada, para lo cual elegiremos una línea aleatoria del texto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua78KRJdYkZk"
      },
      "source": [
        "from random import randint"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EDkPYM5Gdmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e8e869-d7fa-4a61-826b-91ccd83d351a"
      },
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "great utility of having wives and children in common the possibility is quite another matter and will be very much disputed i think that a good many doubts may be raised about both you imply that the two questions must be combined i replied now i meant that you should admit\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZO-Teo5Go9O"
      },
      "source": [
        "A continuación podemos generar nuevas palabras una por una. Primero, el texto debe codificarse usando el tokenizer que hemos cargado anteriormente. Ahora el modelo puede predecir nuevas palabras usando el método ``predict_classes()`` que devuelve el índice de la palabra con probabilidad más alta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mNpLQAtHMEY"
      },
      "source": [
        "Esta palabra se añade a nuestro texto inicial y se repite el proceso. Notar que esta secuencia va a ir creciendo por lo que tendremos que truncarla, para lo que utilizamos la función ``pad_sequences()`` de Keras. Todo este proceso se puede implementar con la siguiente función. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4J29cLaHZx-"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = pad_sequences([encoded],maxlen=seq_length,truncating='pre')\n",
        "        yhat = model.predict_classes(encoded,verbose=0)\n",
        "        out_word = ''\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6dQKHIIIot"
      },
      "source": [
        "Ahora podemos generar una nueva secuencia usando el siguiente código. Cada vez que lo ejecutemos obtendremos un resultado distinto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FUn97EmILPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb3d0e5-4d6c-47ce-da57-30c4c876b8af"
      },
      "source": [
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model,tokenizer,seq_length,seed_text,50)\n",
        "print(generated)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "of payment is medicine i should not nor would you say that medicine is the art of receiving pay because a man takes fees when he is engaged in healing certainly not and we have admitted i said that the good of each art is specially confined to the art yes\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "he said the same principle holds of the other part of the other part of the other part of the other part of the other part of the other part of the other part of the other part of the other degree that resides of the state constitutes courage in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlmSJGumIepK"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Elige tu propio libro del proyecto Gutenberg (es posible usar libros en [español](https://www.gutenberg.org/browse/languages/es)) y crea tu propio modelo de lenguaje. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jsZvvYI21o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227d07a3-8147-4532-fbdf-2323d4c79122"
      },
      "source": [
        "!wget https://www.dropbox.com/s/g9v964y5y2yl94j/regenta.txt?dl=1 -O regenta.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-15 09:57:35--  https://www.dropbox.com/s/g9v964y5y2yl94j/regenta.txt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:601a:18::a27d:712\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/g9v964y5y2yl94j/regenta.txt [following]\n",
            "--2021-05-15 09:57:35--  https://www.dropbox.com/s/dl/g9v964y5y2yl94j/regenta.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com/cd/0/get/BOgdz9XNbccMnwD3k3RQcUWXs7zTzFsu2pwmRP0wlPFe-GhFBMTe_4UokELYiA_Zsv4veeJPiUcFJPFaOTobIi0ExA27ooRlewkr4jQ_uKxGZkcld9l3I8daCJ_BE_ACn97QfnLsA1Pn2HC_EinhWcg6/file?dl=1# [following]\n",
            "--2021-05-15 09:57:35--  https://ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com/cd/0/get/BOgdz9XNbccMnwD3k3RQcUWXs7zTzFsu2pwmRP0wlPFe-GhFBMTe_4UokELYiA_Zsv4veeJPiUcFJPFaOTobIi0ExA27ooRlewkr4jQ_uKxGZkcld9l3I8daCJ_BE_ACn97QfnLsA1Pn2HC_EinhWcg6/file?dl=1\n",
            "Resolving ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com (ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:601a:15::a27d:70f\n",
            "Connecting to ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com (ucd9761b1d63ff3385cbb39b5dc7.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 817151 (798K) [application/binary]\n",
            "Saving to: ‘regenta.txt’\n",
            "\n",
            "regenta.txt         100%[===================>] 798.00K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-05-15 09:57:36 (19.1 MB/s) - ‘regenta.txt’ saved [817151/817151]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7zXCTxXTqfm"
      },
      "source": [
        "**Preparando los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyve9NR6TaAT",
        "outputId": "1c769154-f2c1-47f7-b422-fa435a1a87bb"
      },
      "source": [
        "!head -30 regenta.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Prólogo\r\n",
            "\r\n",
            "\r\n",
            "Creo que fue Wieland quien dijo _que los pensamientos de los hombres\r\n",
            "valen más que sus acciones, y las buenas novelas más que el género\r\n",
            "humano_. Podrá esto no ser verdad; pero es hermoso y consolador.\r\n",
            "Ciertamente, parece que nos ennoblecemos trasladándonos de este mundo al\r\n",
            "otro, de la realidad en que somos tan malos a la ficción en que valemos\r\n",
            "más que aquí, y véase por qué, cuando un cristiano el hábito de pasar\r\n",
            "fácilmente a mejor vida, inventando personas y tejiendo sucesos a imagen\r\n",
            "de los de por acá, le cuesta no poco trabajo volver a este mundo.\r\n",
            "También digo que si grata es la tarea de fabricar género humano\r\n",
            "recreándonos en ver cuánto superan las ideales figurillas, por toscas\r\n",
            "que sean, a las vivas figuronas que a nuestro lado bullen, el regocijo\r\n",
            "es más intenso cuando visitamos los talleres ajenos, pues el andar\r\n",
            "siempre en los propios trae un desasosiego que amengua los placeres de\r\n",
            "lo que llamaremos creación, por no tener mejor nombre que darle.\r\n",
            "\r\n",
            "Esto que digo de visitar talleres ajenos no significa precisamente una\r\n",
            "labor crítica, que si así fuera yo aborrecía tales visitas en vez de\r\n",
            "amarlas; es recrearse en las obras ajenas sabiendo cómo se hacen o cómo\r\n",
            "se intenta su ejecución; es buscar y sorprender las dificultades\r\n",
            "vencidas, los aciertos fáciles o alcanzados con poderoso esfuerzo; es\r\n",
            "buscar y satisfacer uno de los pocos placeres que hay en la vida, la\r\n",
            "admiración, a más de placer, necesidad imperiosa en toda profesión u\r\n",
            "oficio, pues el admirar entendiendo que es la respiración del arte, y el\r\n",
            "que no admira corre el peligro de morir de asfixia.\r\n",
            "\r\n",
            "El estado presente de nuestra cultura, incierto y un tanto enfermizo,\r\n",
            "con desalientos y suspicacias de enfermo de aprensión, nos impone la\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1e-iqrAT_zH"
      },
      "source": [
        "**Cargando los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkdKN961TguQ"
      },
      "source": [
        "in_filename = 'regenta.txt'\n",
        "doc = load_doc(in_filename)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XC8dQB2TkKF",
        "outputId": "6647fb42-348b-4e7c-fd43-286bf2b82440"
      },
      "source": [
        "print(doc[:200])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Prólogo\n",
            "\n",
            "\n",
            "Creo que fue Wieland quien dijo _que los pensamientos de los hombres\n",
            "valen más que sus acciones, y las buenas novelas más que el género\n",
            "humano_. Podrá esto no ser verdad; pero es hermoso y \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVOFFv8sTt_B"
      },
      "source": [
        "**Limpiando los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1ZIuqe_T3wd",
        "outputId": "f6fbbcc8-d685-4b0b-a5db-f194884004b2"
      },
      "source": [
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['creo', 'que', 'fue', 'wieland', 'quien', 'dijo', 'que', 'los', 'pensamientos', 'de', 'los', 'hombres', 'valen', 'más', 'que', 'sus', 'acciones', 'y', 'las', 'buenas', 'novelas', 'más', 'que', 'el', 'género', 'humano', 'podrá', 'esto', 'no', 'ser', 'verdad', 'pero', 'es', 'hermoso', 'y', 'consolador', 'ciertamente', 'parece', 'que', 'nos', 'ennoblecemos', 'trasladándonos', 'de', 'este', 'mundo', 'al', 'otro', 'de', 'la', 'realidad', 'en', 'que', 'somos', 'tan', 'malos', 'a', 'la', 'ficción', 'en', 'que', 'valemos', 'más', 'que', 'aquí', 'y', 'véase', 'por', 'qué', 'cuando', 'un', 'cristiano', 'el', 'hábito', 'de', 'pasar', 'fácilmente', 'a', 'mejor', 'vida', 'inventando', 'personas', 'y', 'tejiendo', 'sucesos', 'a', 'imagen', 'de', 'los', 'de', 'por', 'acá', 'le', 'cuesta', 'no', 'poco', 'trabajo', 'volver', 'a', 'este', 'mundo', 'también', 'digo', 'que', 'si', 'grata', 'es', 'la', 'tarea', 'de', 'fabricar', 'género', 'humano', 'recreándonos', 'en', 'ver', 'cuánto', 'superan', 'las', 'ideales', 'figurillas', 'por', 'toscas', 'que', 'sean', 'a', 'las', 'vivas', 'figuronas', 'que', 'a', 'nuestro', 'lado', 'bullen', 'el', 'regocijo', 'es', 'más', 'intenso', 'cuando', 'visitamos', 'los', 'talleres', 'ajenos', 'pues', 'el', 'andar', 'siempre', 'en', 'los', 'propios', 'trae', 'un', 'desasosiego', 'que', 'amengua', 'los', 'placeres', 'de', 'lo', 'que', 'llamaremos', 'creación', 'por', 'no', 'tener', 'mejor', 'nombre', 'que', 'darle', 'esto', 'que', 'digo', 'de', 'visitar', 'talleres', 'ajenos', 'no', 'significa', 'precisamente', 'una', 'labor', 'crítica', 'que', 'si', 'así', 'fuera', 'yo', 'aborrecía', 'tales', 'visitas', 'en', 'vez', 'de', 'amarlas', 'es', 'recrearse', 'en', 'las', 'obras', 'ajenas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdemlk2jULNz",
        "outputId": "fb742a4f-19c3-4994-874d-e4495cf5a36c"
      },
      "source": [
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' %len(set(tokens)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 131766\n",
            "Unique Tokens: 15467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvM-Kag7UMsl"
      },
      "source": [
        "Es decir, nuestro modelo consta de unas 15500 palabras. Este tamaño de vocabulario es manejable (recortando un tercio de la obra original por una limitación de memoria RAM). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hJltHYPWM4r"
      },
      "source": [
        "**Guardando los datos limpios**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxwgNnb3WPtj"
      },
      "source": [
        "lines = organize_tokens(tokens)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OyNMGKgWVv4"
      },
      "source": [
        "out_filename = 'regenta_sequences.txt'\n",
        "save_doc(lines, out_filename)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cwuvrl0WrGX",
        "outputId": "f6b2e441-e86c-4baa-e790-6b2ead352f38"
      },
      "source": [
        "!head -5 regenta_sequences.txt"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creo que fue wieland quien dijo que los pensamientos de los hombres valen más que sus acciones y las buenas novelas más que el género humano podrá esto no ser verdad pero es hermoso y consolador ciertamente parece que nos ennoblecemos trasladándonos de este mundo al otro de la realidad en\n",
            "que fue wieland quien dijo que los pensamientos de los hombres valen más que sus acciones y las buenas novelas más que el género humano podrá esto no ser verdad pero es hermoso y consolador ciertamente parece que nos ennoblecemos trasladándonos de este mundo al otro de la realidad en que\n",
            "fue wieland quien dijo que los pensamientos de los hombres valen más que sus acciones y las buenas novelas más que el género humano podrá esto no ser verdad pero es hermoso y consolador ciertamente parece que nos ennoblecemos trasladándonos de este mundo al otro de la realidad en que somos\n",
            "wieland quien dijo que los pensamientos de los hombres valen más que sus acciones y las buenas novelas más que el género humano podrá esto no ser verdad pero es hermoso y consolador ciertamente parece que nos ennoblecemos trasladándonos de este mundo al otro de la realidad en que somos tan\n",
            "quien dijo que los pensamientos de los hombres valen más que sus acciones y las buenas novelas más que el género humano podrá esto no ser verdad pero es hermoso y consolador ciertamente parece que nos ennoblecemos trasladándonos de este mundo al otro de la realidad en que somos tan malos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAFfFt6sXWB6"
      },
      "source": [
        "**Entrenando el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z7PU1pfXYJO"
      },
      "source": [
        "in_filename = 'regenta_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MBPWtfqXhQI"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEICbAt4W8NH"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZo4I5jXXv1"
      },
      "source": [
        "sequences = array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htnfD-6hZHWd",
        "outputId": "00e03a8e-8b30-4714-d4a3-ab499d521068"
      },
      "source": [
        "model = define_model(vocab_size, seq_length)\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1030/1030 [==============================] - 57s 25ms/step - loss: 7.3158 - accuracy: 0.0514\n",
            "Epoch 2/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 6.5169 - accuracy: 0.0751\n",
            "Epoch 3/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 6.2779 - accuracy: 0.0837\n",
            "Epoch 4/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 6.0902 - accuracy: 0.0968\n",
            "Epoch 5/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 5.9411 - accuracy: 0.1028\n",
            "Epoch 6/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 5.7954 - accuracy: 0.1071\n",
            "Epoch 7/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 5.6472 - accuracy: 0.1109\n",
            "Epoch 8/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 5.5158 - accuracy: 0.1163\n",
            "Epoch 9/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 5.4175 - accuracy: 0.1221\n",
            "Epoch 10/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 5.3171 - accuracy: 0.1274\n",
            "Epoch 11/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 5.2197 - accuracy: 0.1342\n",
            "Epoch 12/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 5.1492 - accuracy: 0.1364\n",
            "Epoch 13/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 5.0781 - accuracy: 0.1404\n",
            "Epoch 14/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 4.9999 - accuracy: 0.1427\n",
            "Epoch 15/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.9261 - accuracy: 0.1444\n",
            "Epoch 16/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.8429 - accuracy: 0.1498\n",
            "Epoch 17/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.7716 - accuracy: 0.1527\n",
            "Epoch 18/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.7036 - accuracy: 0.1558\n",
            "Epoch 19/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.6404 - accuracy: 0.1595\n",
            "Epoch 20/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.6003 - accuracy: 0.1608\n",
            "Epoch 21/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 4.5329 - accuracy: 0.1639\n",
            "Epoch 22/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.4847 - accuracy: 0.1664\n",
            "Epoch 23/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.4288 - accuracy: 0.1716\n",
            "Epoch 24/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.3881 - accuracy: 0.1739\n",
            "Epoch 25/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.3433 - accuracy: 0.1771\n",
            "Epoch 26/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.3142 - accuracy: 0.1810\n",
            "Epoch 27/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.2851 - accuracy: 0.1809\n",
            "Epoch 28/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.2305 - accuracy: 0.1867\n",
            "Epoch 29/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.2011 - accuracy: 0.1903\n",
            "Epoch 30/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.1679 - accuracy: 0.1925\n",
            "Epoch 31/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.1373 - accuracy: 0.1971\n",
            "Epoch 32/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.0873 - accuracy: 0.2010\n",
            "Epoch 33/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 4.0646 - accuracy: 0.2047\n",
            "Epoch 34/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 4.0318 - accuracy: 0.2061\n",
            "Epoch 35/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.9941 - accuracy: 0.2119\n",
            "Epoch 36/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.9620 - accuracy: 0.2157\n",
            "Epoch 37/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.9246 - accuracy: 0.2217\n",
            "Epoch 38/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.8907 - accuracy: 0.2247\n",
            "Epoch 39/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.8647 - accuracy: 0.2281\n",
            "Epoch 40/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.8466 - accuracy: 0.2287\n",
            "Epoch 41/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.8166 - accuracy: 0.2341\n",
            "Epoch 42/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.8018 - accuracy: 0.2362\n",
            "Epoch 43/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.7644 - accuracy: 0.2398\n",
            "Epoch 44/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.7617 - accuracy: 0.2411\n",
            "Epoch 45/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.7320 - accuracy: 0.2443\n",
            "Epoch 46/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.6850 - accuracy: 0.2517\n",
            "Epoch 47/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.6735 - accuracy: 0.2535\n",
            "Epoch 48/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.6633 - accuracy: 0.2538\n",
            "Epoch 49/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.6180 - accuracy: 0.2586\n",
            "Epoch 50/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.6000 - accuracy: 0.2662\n",
            "Epoch 51/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.5847 - accuracy: 0.2659\n",
            "Epoch 52/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.5625 - accuracy: 0.2671\n",
            "Epoch 53/100\n",
            "1030/1030 [==============================] - 25s 24ms/step - loss: 3.5488 - accuracy: 0.2711\n",
            "Epoch 54/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.5109 - accuracy: 0.2772\n",
            "Epoch 55/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.5017 - accuracy: 0.2764\n",
            "Epoch 56/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.4647 - accuracy: 0.2844\n",
            "Epoch 57/100\n",
            "1030/1030 [==============================] - 25s 25ms/step - loss: 3.4422 - accuracy: 0.2895\n",
            "Epoch 58/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.4408 - accuracy: 0.2861\n",
            "Epoch 59/100\n",
            "1030/1030 [==============================] - 27s 26ms/step - loss: 3.4063 - accuracy: 0.2918\n",
            "Epoch 60/100\n",
            "1030/1030 [==============================] - 26s 26ms/step - loss: 3.4027 - accuracy: 0.2936\n",
            "Epoch 61/100\n",
            "1030/1030 [==============================] - 27s 26ms/step - loss: 3.3734 - accuracy: 0.2975\n",
            "Epoch 62/100\n",
            "1030/1030 [==============================] - 27s 26ms/step - loss: 3.3492 - accuracy: 0.2996\n",
            "Epoch 63/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.3329 - accuracy: 0.3051\n",
            "Epoch 64/100\n",
            "1030/1030 [==============================] - 26s 26ms/step - loss: 3.3172 - accuracy: 0.3076\n",
            "Epoch 65/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2968 - accuracy: 0.3086\n",
            "Epoch 66/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2805 - accuracy: 0.3139\n",
            "Epoch 67/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2606 - accuracy: 0.3149\n",
            "Epoch 68/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2305 - accuracy: 0.3202\n",
            "Epoch 69/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2233 - accuracy: 0.3230\n",
            "Epoch 70/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.2110 - accuracy: 0.3216\n",
            "Epoch 71/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1967 - accuracy: 0.3257\n",
            "Epoch 72/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1744 - accuracy: 0.3308\n",
            "Epoch 73/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1620 - accuracy: 0.3330\n",
            "Epoch 74/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1525 - accuracy: 0.3331\n",
            "Epoch 75/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1048 - accuracy: 0.3428\n",
            "Epoch 76/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.0961 - accuracy: 0.3425\n",
            "Epoch 77/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.1085 - accuracy: 0.3411\n",
            "Epoch 78/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.0727 - accuracy: 0.3481\n",
            "Epoch 79/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.0489 - accuracy: 0.3512\n",
            "Epoch 80/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.0516 - accuracy: 0.3507\n",
            "Epoch 81/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 3.0155 - accuracy: 0.3554\n",
            "Epoch 82/100\n",
            "1030/1030 [==============================] - 26s 26ms/step - loss: 3.0037 - accuracy: 0.3582\n",
            "Epoch 83/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9971 - accuracy: 0.3594\n",
            "Epoch 84/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9822 - accuracy: 0.3639\n",
            "Epoch 85/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9572 - accuracy: 0.3679\n",
            "Epoch 86/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9580 - accuracy: 0.3661\n",
            "Epoch 87/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9254 - accuracy: 0.3721\n",
            "Epoch 88/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9332 - accuracy: 0.3719\n",
            "Epoch 89/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.9033 - accuracy: 0.3777\n",
            "Epoch 90/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8961 - accuracy: 0.3774\n",
            "Epoch 91/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8817 - accuracy: 0.3796\n",
            "Epoch 92/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8772 - accuracy: 0.3830\n",
            "Epoch 93/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8510 - accuracy: 0.3890\n",
            "Epoch 94/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8485 - accuracy: 0.3854\n",
            "Epoch 95/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8241 - accuracy: 0.3913\n",
            "Epoch 96/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.8230 - accuracy: 0.3915\n",
            "Epoch 97/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.7917 - accuracy: 0.3972\n",
            "Epoch 98/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.7893 - accuracy: 0.3977\n",
            "Epoch 99/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.7602 - accuracy: 0.4014\n",
            "Epoch 100/100\n",
            "1030/1030 [==============================] - 26s 25ms/step - loss: 2.7476 - accuracy: 0.4034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbed904c790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGIuSazvZplb"
      },
      "source": [
        "model.save_weights('./model2.h5', overwrite=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYdkiq71Zp9j"
      },
      "source": [
        "dump(tokenizer, open('tokenizer2.pkl','wb'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS_lA3rEZJje"
      },
      "source": [
        "**Usando el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBsXN8kfZUn0"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_zAJgQnaAIA",
        "outputId": "4730d0c6-5059-4f6e-bc31-efbdc33a687a"
      },
      "source": [
        "seed_text = lines[randint(0, len(lines))]\n",
        "print(seed_text + '\\n')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "salgo estas palabras apenas dichas le parecieron imprudentes ella quien las había pronunciado así hablaba obdulia con los hombres ella ana don álvaro se vio en un apuro pretendía aquella señora una conversación para aludir a lo que había entre ellos que en rigor no era nada que mereciese comentarios él\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg3XcPnYaEs9",
        "outputId": "5fbb5225-c6fe-4a2d-9dd2-48f991fa8e82"
      },
      "source": [
        "seed_text = lines[randint(0, len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jugaban al tute la presencia del provisor interrumpió el juego los familiares se pusieron de pie y uno de ellos hermoso rubio de movimientos suaves y ondulantes de pulquérrimo traje talar perfumado abrió una mampara forrada de damasco color cereza de lo mismo estaba tapizada toda la estancia que se vio\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "a solas y se le antojaba a la marquesa estando decidida bien trepando poco a mí y se le antojaba de tus precauciones predominaba un castigo de la estación y con poder que para cazar gorriones no es la muerte de la catedral y la regenta se le había figurado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG188-6TI3ph"
      },
      "source": [
        "Recuerda guardar este notebook en tu repositorio usando la opción \"Save in GitHub\" del menú File."
      ]
    }
  ]
}